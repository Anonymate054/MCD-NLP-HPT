{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"ec67bcd310ce482f9a46f9d55432c449","deepnote_cell_type":"markdown"},"source":"# Preparing the data","block_group":"13ce7c050cc54d909015908019c84eed"},{"cell_type":"code","metadata":{"source_hash":"f61b930d","execution_start":1733896398226,"execution_millis":1,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"90a53e47ccfc402bb63627e984c06a89","deepnote_cell_type":"code"},"source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom nltk.probability import FreqDist\nimport pandas as pd\nimport ast\nimport fs","block_group":"63e0f8ce2da542ea84ae861ee5b5c5c1","execution_count":3,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"647472ee","execution_start":1733897383298,"execution_millis":0,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"a96b61bd2d304472bce3bba098123c86","deepnote_cell_type":"code"},"source":"PROCESSED_FILES_DIR = fs.open_fs(\"MCD-NLP-HPT/data/processed\")\nTRAIN_DIR = PROCESSED_FILES_DIR.getsyspath(\"train.csv\")\nTEST_DIR = PROCESSED_FILES_DIR.getsyspath(\"test.csv\")\nVAL_DIR = PROCESSED_FILES_DIR.getsyspath(\"validation.csv\")\n\nINTERIM_COMMENTS_CLEANED_ONLY_SW_DIR = INTERIM_FILES_DIR.getsyspath(\"Comments_cleaned_only_sw.csv\")\nTRAIN_ONLY_SW_DIR = PROCESSED_FILES_DIR.getsyspath(\"train_only_sw.csv\")\nTEST_ONLY_SW_DIR = PROCESSED_FILES_DIR.getsyspath(\"test_only_sw.csv\")\nVAL_ONLY_SW_DIR = PROCESSED_FILES_DIR.getsyspath(\"validation_only_sw.csv\")","block_group":"8cbe80a94fb5478d9726dabd8b8b134f","execution_count":17,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"e25864d785f24e4094721c45a8e94685","deepnote_cell_type":"markdown"},"source":"### Clase principal para el algoritmo\n\nRecuerda que la clase m√°s probable viene dada por (en espacio de c√≥mputo logar√≠tmico): \n\n\n$$\\hat{c} = {\\arg \\max}_{(c)}\\log{P(c)}\n +\\sum_{i=1}^n\n\\log{ P(f_i \\vert c)}\n$$\n\nDonde, para evitar casos at√≠picos, usaremos el suavizado de Laplace as√≠:\n\n$$\nP(f_i \\vert c) = \\frac{C(f_i, c)+1}{C(c) + \\vert V \\vert}\n$$\n\nsiendo $\\vert V \\vert$ la longitud del vocabulario de nuestro conjunto de entrenamiento. ","block_group":"731aea3f5cde46498fd9615ca4692c75"},{"cell_type":"code","metadata":{"source_hash":"35166b4","execution_start":1733897413162,"execution_millis":0,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"50b059ebeab54b418a67a364f03bb89e","deepnote_cell_type":"code"},"source":"import math\nfrom collections import defaultdict\nfrom nltk.probability import FreqDist\nimport numpy as np\n\n\nclass NaiveBayesClassifier:\n    def __init__(self):\n        self.unique_classes = set()\n        self.vocab = set()\n        self.class_count = {}  # C(c)\n        self.log_class_prior_prob = {}  # P(c)\n        self.word_conditional_counts = defaultdict(lambda: defaultdict(float))  # C(w|c)\n\n    def fit(self, x_data, y_data):\n        \"\"\"\n        Train the Naive Bayes Classifier on the given data.\n        :param x_data: List of tokenized texts.\n        :param y_data: List of class labels corresponding to x_data.\n        \"\"\"\n        num_samples = len(x_data)\n        self.unique_classes = set(y_data)\n\n        # Count occurrences of each class\n        for cls in y_data:\n            self.class_count[cls] = self.class_count.get(cls, 0) + 1\n\n        # Compute log prior probabilities for each class\n        for cls, count in self.class_count.items():\n            self.log_class_prior_prob[cls] = math.log(count / num_samples)\n\n        # Count word occurrences per class\n        for tokens, cls in zip(x_data, y_data):\n            counts = FreqDist(tokens)\n            for word, count in counts.items():\n                self.vocab.add(word)\n                self.word_conditional_counts[cls][word] += count\n\n    def predict(self, data):\n        \"\"\"\n        Predict class labels for the given data.\n        :param data: List of tokenized texts.\n        :return: List of predicted class labels.\n        \"\"\"\n        results = []\n\n        for text in data:\n            words = set(FreqDist(text))  # Unique words in the text\n            score_prob = {}\n\n            for cls in self.unique_classes:\n                # Initialize with prior probability\n                score_prob[cls] = self.log_class_prior_prob[cls]\n\n                for word in words:\n                    if word in self.vocab:\n                        # Laplace smoothing\n                        word_count = self.word_conditional_counts[cls].get(word, 0.0)\n                        log_word_prob = math.log(\n                            (word_count + 1) / (self.class_count[cls] + len(self.vocab))\n                        )\n                        score_prob[cls] += log_word_prob\n\n            # Find the class with the highest score\n            predicted_class = max(score_prob, key=score_prob.get)\n            results.append(predicted_class)\n\n        return results","block_group":"c0e358e7a003495f9bcbc1d9d8719499","execution_count":18,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"bd0c498b","execution_start":1733896416894,"execution_millis":1189,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"905920328f154263a9231915f260a1c7","deepnote_cell_type":"code"},"source":"df_train = pd.read_csv(TRAIN_DIR)\ndf_test = pd.read_csv(TEST_DIR)\ndf_val = pd.read_csv(VAL_DIR)","block_group":"62bbf98a61c64af79affa79cd94fa606","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"b122535b","execution_start":1733897431314,"execution_millis":620,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"24b3415ce2a94bfdb0764ae9ce9d61cf","deepnote_cell_type":"code"},"source":"df_train_2 = pd.read_csv(TRAIN_ONLY_SW_DIR)\ndf_test_2 = pd.read_csv(TEST_ONLY_SW_DIR)\ndf_val_2 = pd.read_csv(VAL_ONLY_SW_DIR)","block_group":"cf4a69cb643d433a85e408fff4fdac07","execution_count":19,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"591d3b8d","execution_start":1733896418246,"execution_millis":7,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0,"hiddenColumnIds":[],"conditionalFilters":[],"cellFormattingRules":[],"wrappedTextColumnIds":[]},"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","deepnote_table_loading":false,"cell_id":"b5b35a61c2814a52894342e097b8ded1","deepnote_cell_type":"code"},"source":"df_train","block_group":"705d936ea4574e0cade471ecf4f9f2fd","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":14,"row_count":922,"columns":[{"name":"id","dtype":"object","stats":{"unique_count":922,"nan_count":0,"categories":[{"name":"UgwtT3n4ZD6aX2c3NrF4AaABAg","count":1},{"name":"Ugzlbev9x6glV6YNpyN4AaABAg","count":1},{"name":"920 others","count":920}]}},{"name":"videoId","dtype":"object","stats":{"unique_count":25,"nan_count":0,"categories":[{"name":"RBnZPFhDcnk","count":76},{"name":"ShW6FY-vbmo","count":53},{"name":"23 others","count":793}]}},{"name":"textOriginal","dtype":"object","stats":{"unique_count":908,"nan_count":0,"categories":[{"name":"Imperio japon√©s pt4 plis","count":12},{"name":"Este cap√≠tulo debi√≥ llamarse las flipantes aventuras del reinado de hispania. Ol√©!","count":2},{"name":"906 others","count":908}]}},{"name":"authorDisplayName","dtype":"object","stats":{"unique_count":634,"nan_count":8,"categories":[{"name":"@ivanmelchor2889","count":14},{"name":"633 others","count":900},{"name":"Missing","count":8}]}},{"name":"likeCount","dtype":"int64","stats":{"unique_count":23,"nan_count":0,"min":"0","max":"43","histogram":[{"bin_start":0,"bin_end":4.3,"count":870},{"bin_start":4.3,"bin_end":8.6,"count":27},{"bin_start":8.6,"bin_end":12.899999999999999,"count":11},{"bin_start":12.899999999999999,"bin_end":17.2,"count":4},{"bin_start":17.2,"bin_end":21.5,"count":3},{"bin_start":21.5,"bin_end":25.799999999999997,"count":4},{"bin_start":25.799999999999997,"bin_end":30.099999999999998,"count":1},{"bin_start":30.099999999999998,"bin_end":34.4,"count":1},{"bin_start":34.4,"bin_end":38.699999999999996,"count":0},{"bin_start":38.699999999999996,"bin_end":43,"count":1}]}},{"name":"publishedAt","dtype":"object","stats":{"unique_count":922,"nan_count":0,"categories":[{"name":"2024-04-18T13:32:34Z","count":1},{"name":"2024-06-26T05:30:57Z","count":1},{"name":"920 others","count":920}]}},{"name":"category_id","dtype":"int64","stats":{"unique_count":7,"nan_count":0,"min":"1","max":"7","histogram":[{"bin_start":1,"bin_end":1.6,"count":82},{"bin_start":1.6,"bin_end":2.2,"count":151},{"bin_start":2.2,"bin_end":2.8,"count":0},{"bin_start":2.8,"bin_end":3.4,"count":62},{"bin_start":3.4,"bin_end":4,"count":0},{"bin_start":4,"bin_end":4.6,"count":115},{"bin_start":4.6,"bin_end":5.2,"count":194},{"bin_start":5.2,"bin_end":5.8,"count":0},{"bin_start":5.8,"bin_end":6.3999999999999995,"count":206},{"bin_start":6.3999999999999995,"bin_end":7,"count":112}]}},{"name":"category_description","dtype":"object","stats":{"unique_count":7,"nan_count":0,"categories":[{"name":"Comentarios humor√≠sticos o memes","count":206},{"name":"Felicitaciones y agradecimientos","count":194},{"name":"5 others","count":522}]}},{"name":"Tokens_full","dtype":"object","stats":{"unique_count":904,"nan_count":0,"categories":[{"name":"['Imperio', 'japon√©s', 'pt4', 'plis']","count":12},{"name":"[]","count":6},{"name":"902 others","count":904}]}},{"name":"Tokens","dtype":"object","stats":{"unique_count":904,"nan_count":0,"categories":[{"name":"['Imperio', 'japon√©s', 'pt4', 'plis']","count":12},{"name":"[]","count":6},{"name":"902 others","count":904}]}},{"name":"Tokens_without_stopwords","dtype":"object","stats":{"unique_count":903,"nan_count":0,"categories":[{"name":"['Imperio', 'japon√©s', 'pt4', 'plis']","count":12},{"name":"[]","count":6},{"name":"901 others","count":904}]}},{"name":"Tokens_without_stopwords_stemmed","dtype":"object","stats":{"unique_count":902,"nan_count":0,"categories":[{"name":"['imperi', 'japones', 'pt4', 'plis']","count":12},{"name":"[]","count":6},{"name":"900 others","count":904}]}},{"name":"Tokens_lemmatized","dtype":"object","stats":{"unique_count":908,"nan_count":0,"categories":[{"name":"['imperio', 'japon√©s', 'pt4', 'plis']","count":12},{"name":"['este', 'cap√≠tulo', 'deber', 'llamar', '√©l', 'el', 'flipante', 'aventura', 'de', 'el', 'reinado', 'de', 'hispania', '.', 'ol√©', '!']","count":2},{"name":"906 others","count":908}]}},{"name":"Tokens_without_stopwords_lemmatized","dtype":"object","stats":{"unique_count":903,"nan_count":0,"categories":[{"name":"['imperio', 'japon√©s', 'pt4', 'plis']","count":12},{"name":"[]","count":6},{"name":"901 others","count":904}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":"UgwtT3n4ZD6aX2c3NrF4AaABAg","videoId":"FXxcNjTZ4qo","textOriginal":"No es un mito, Paraguay üáµüáæ si existe.....","authorDisplayName":"@rosaelenarolonespinosa4666","likeCount":1,"publishedAt":"2024-04-18T13:32:34Z","category_id":7,"category_description":"Comentarios generales","Tokens_full":"['No', 'es', 'un', 'mito', ',', 'Paraguay', 'si', 'existe', '...', '.', '.']","Tokens":"['No', 'es', 'un', 'mito', 'Paraguay', 'si', 'existe', '...']","Tokens_without_stopwords":"['No', 'mito', 'Paraguay', 'si', 'existe', '...']","Tokens_without_stopwords_stemmed":"['no', 'mit', 'paraguay', 'si', 'exist', '...']","Tokens_lemmatized":"['no', 'ser', 'uno', 'mito', ',', 'paraguay', 'üáµüáæ', 'si', 'existir', '.....']","Tokens_without_stopwords_lemmatized":"['no', 'mito', 'paraguay', 'si', 'existir', '...']","_deepnote_index_column":0},{"id":"Ugzlbev9x6glV6YNpyN4AaABAg","videoId":"CPCN1Lqzc2U","textOriginal":"Un podcast sobre la radio con DIANA URIBE, la mejor radio historiadora de Latinoam√©rica ‚ù§","authorDisplayName":"@KarimVG","likeCount":0,"publishedAt":"2024-06-26T05:30:57Z","category_id":5,"category_description":"Felicitaciones y agradecimientos","Tokens_full":"['Un', 'podcast', 'sobre', 'la', 'radio', 'con', 'DIANA', 'URIBE', ',', 'la', 'mejor', 'radio', 'historiadora', 'de', 'Latinoam√©rica']","Tokens":"['Un', 'podcast', 'sobre', 'la', 'radio', 'con', 'DIANA', 'URIBE', 'la', 'mejor', 'radio', 'historiadora', 'de', 'Latinoam√©rica']","Tokens_without_stopwords":"['Un', 'podcast', 'radio', 'DIANA', 'URIBE', 'mejor', 'radio', 'historiadora', 'Latinoam√©rica']","Tokens_without_stopwords_stemmed":"['un', 'podcast', 'radi', 'dian', 'urib', 'mejor', 'radi', 'histori', 'latinoamer']","Tokens_lemmatized":"['uno', 'podcast', 'sobre', 'el', 'radio', 'con', 'diana', 'URIBE', ',', 'el', 'mejor', 'radio', 'historiador', 'de', 'latinoam√©rica', '‚ù§']","Tokens_without_stopwords_lemmatized":"['uno', 'podcast', 'radio', 'diana', 'URIBE', 'mejor', 'radio', 'historiadora', 'latinoam√©rica']","_deepnote_index_column":1},{"id":"UgzIKrDRdcGk13aMtul4AaABAg","videoId":"ShW6FY-vbmo","textOriginal":"Cada vez que los escucho, muero de risa y me dan ganas de abrir una cervecita. Saludos desde Rep√∫blica Dominicana.","authorDisplayName":"@ceciliamendiola8646","likeCount":2,"publishedAt":"2024-06-18T21:15:49Z","category_id":6,"category_description":"Comentarios humor√≠sticos o memes","Tokens_full":"['Cada', 'vez', 'que', 'los', 'escucho', ',', 'muero', 'de', 'risa', 'y', 'me', 'dan', 'ganas', 'de', 'abrir', 'una', 'cervecita', '.', 'Saludos', 'desde', 'Rep√∫blica', 'Dominicana', '.']","Tokens":"['Cada', 'vez', 'que', 'los', 'escucho', 'muero', 'de', 'risa', 'y', 'me', 'dan', 'ganas', 'de', 'abrir', 'una', 'cervecita', 'Saludos', 'desde', 'Rep√∫blica', 'Dominicana']","Tokens_without_stopwords":"['Cada', 'vez', 'escucho', 'muero', 'risa', 'dan', 'ganas', 'abrir', 'cervecita', 'Saludos', 'Rep√∫blica', 'Dominicana']","Tokens_without_stopwords_stemmed":"['cad', 'vez', 'escuch', 'muer', 'ris', 'dan', 'gan', 'abrir', 'cervecit', 'salud', 'republ', 'dominican']","Tokens_lemmatized":"['cada', 'vez', 'que', '√©l', 'escuchar', ',', 'muero', 'de', 'risa', 'y', 'yo', 'dar', 'gana', 'de', 'abrir', 'uno', 'cervecito', '.', 'saludo', 'desde', 'rep√∫blica', 'Dominicana', '.']","Tokens_without_stopwords_lemmatized":"['cada', 'vez', 'escuchar', 'muero', 'risa', 'dar', 'gana', 'abrir', 'cervecito', 'saludo', 'rep√∫blica', 'Dominicana']","_deepnote_index_column":2},{"id":"Ugz5Q5R5ls4pXTXSi294AaABAg","videoId":"HxMXLWqe9HQ","textOriginal":"Que buen capitulo, son geniales! El cristo del corcovado en mood esa no es tu familia jajaja üòÇ como hab√≠a que hacer para pedirles su intinerario de Jap√≥n?","authorDisplayName":"@anaberthalyochoaporras8262","likeCount":0,"publishedAt":"2024-09-19T04:47:09Z","category_id":5,"category_description":"Felicitaciones y agradecimientos","Tokens_full":"['Que', 'buen', 'capitulo', ',', 'son', 'geniales', 'El', 'cristo', 'del', 'corcovado', 'en', 'mood', 'esa', 'no', 'es', 'tu', 'familia', 'jajaja', 'como', 'hab√≠a', 'que', 'hacer', 'para', 'pedirles', 'su', 'intinerario', 'de', 'Jap√≥n', '?']","Tokens":"['Que', 'buen', 'capitulo', 'son', 'geniales', 'El', 'cristo', 'del', 'corcovado', 'en', 'mood', 'esa', 'no', 'es', 'tu', 'familia', 'jajaja', 'como', 'hab√≠a', 'que', 'hacer', 'para', 'pedirles', 'su', 'intinerario', 'de', 'Jap√≥n']","Tokens_without_stopwords":"['Que', 'buen', 'capitulo', 'geniales', 'El', 'cristo', 'corcovado', 'mood', 'familia', 'jajaja', 'hacer', 'pedirles', 'intinerario', 'Jap√≥n']","Tokens_without_stopwords_stemmed":"['que', 'buen', 'capitul', 'genial', 'el', 'crist', 'corcov', 'mood', 'famili', 'jajaj', 'hac', 'ped', 'intinerari', 'japon']","Tokens_lemmatized":"['que', 'buen', 'capitulo', ',', 'ser', 'genial', '!', 'el', 'cristo', 'de', 'el', 'corcovado', 'en', 'mood', 'ese', 'no', 'ser', 'tu', 'familia', 'jajajar', 'üòÇ', 'como', 'haber', 'que', 'hacer', 'para', 'pedir', '√©l', 'su', 'intinerario', 'de', 'Jap√≥n', '?']","Tokens_without_stopwords_lemmatized":"['que', 'buen', 'capitulo', 'genial', 'el', 'cristo', 'corcovado', 'mood', 'familia', 'jajajar', 'hacer', 'pedir', 'intinerario', 'Jap√≥n']","_deepnote_index_column":3},{"id":"UgwKlLDQnWVL_LOpE1d4AaABAg","videoId":"wCm4FNSnDPs","textOriginal":"Gracias por el podcast de la semana, mis amores. Los extra√±√© ‚ù§","authorDisplayName":"@LadySkywalkerW","likeCount":0,"publishedAt":"2024-10-11T03:38:25Z","category_id":5,"category_description":"Felicitaciones y agradecimientos","Tokens_full":"['Gracias', 'por', 'el', 'podcast', 'de', 'la', 'semana', ',', 'mis', 'amores', '.', 'Los', 'extra√±√©']","Tokens":"['Gracias', 'por', 'el', 'podcast', 'de', 'la', 'semana', 'mis', 'amores', 'Los', 'extra√±√©']","Tokens_without_stopwords":"['Gracias', 'podcast', 'semana', 'amores', 'Los', 'extra√±√©']","Tokens_without_stopwords_stemmed":"['graci', 'podcast', 'seman', 'amor', 'los', 'extra√±']","Tokens_lemmatized":"['gracia', 'por', 'el', 'podcast', 'de', 'el', 'semana', ',', 'mi', 'amor', '.', '√©l', 'extra√±ar', '‚ù§']","Tokens_without_stopwords_lemmatized":"['gracia', 'podcast', 'semana', 'amor', 'el', 'extra√±ar']","_deepnote_index_column":4},{"id":"UgxFgSY23PWWMQfjUqt4AaABAg","videoId":"CPCN1Lqzc2U","textOriginal":"Los paraguayos llevamos toda la vida tenido una relaci√≥n como de perros y gatos con los argentinos cuando en realidad deber√≠amos tener esa relaci√≥n con los Brasile√±os jajaj, es broma üòâ \nSon cosas que tenemos que dejar donde est√°n, que es el PASADO y que ojal√° no se vuelva a repetir nunca xq si vuelve a pasar eso ahora, Paraguay si que desaparecer√≠a, la econom√≠a que tuvimos antes del guerra de la triple alianza jam√°s volvi√≥ ü•≤\nUn saludo ü´∂üèªsiempre los escucho en el trabajo.","authorDisplayName":"@lauraflores7242","likeCount":2,"publishedAt":"2024-06-27T14:03:52Z","category_id":3,"category_description":"Experiencias personales","Tokens_full":"['Los', 'paraguayos', 'llevamos', 'toda', 'la', 'vida', 'tenido', 'una', 'relaci√≥n', 'como', 'de', 'perros', 'y', 'gatos', 'con', 'los', 'argentinos', 'cuando', 'en', 'realidad', 'deber√≠amos', 'tener', 'esa', 'relaci√≥n', 'con', 'los', 'Brasile√±os', 'jajaj', ',', 'es', 'broma', 'Son', 'cosas', 'que', 'tenemos', 'que', 'dejar', 'donde', 'est√°n', ',', 'que', 'es', 'el', 'PASADO', 'y', 'que', 'ojal√°', 'no', 'se', 'vuelva', 'a', 'repetir', 'nunca', 'xq', 'si', 'vuelve', 'a', 'pasar', 'eso', 'ahora', ',', 'Paraguay', 'si', 'que', 'desaparecer√≠a', ',', 'la', 'econom√≠a', 'que', 'tuvimos', 'antes', 'del', 'guerra', 'de', 'la', 'triple', 'alianza', 'jam√°s', 'volvi√≥', 'Un', 'saludo', 'siempre', 'los', 'escucho', 'en', 'el', 'trabajo', '.']","Tokens":"['Los', 'paraguayos', 'llevamos', 'toda', 'la', 'vida', 'tenido', 'una', 'relaci√≥n', 'como', 'de', 'perros', 'y', 'gatos', 'con', 'los', 'argentinos', 'cuando', 'en', 'realidad', 'deber√≠amos', 'tener', 'esa', 'relaci√≥n', 'con', 'los', 'Brasile√±os', 'jajaj', 'es', 'broma', 'Son', 'cosas', 'que', 'tenemos', 'que', 'dejar', 'donde', 'est√°n', 'que', 'es', 'el', 'PASADO', 'y', 'que', 'ojal√°', 'no', 'se', 'vuelva', 'a', 'repetir', 'nunca', 'xq', 'si', 'vuelve', 'a', 'pasar', 'eso', 'ahora', 'Paraguay', 'si', 'que', 'desaparecer√≠a', 'la', 'econom√≠a', 'que', 'tuvimos', 'antes', 'del', 'guerra', 'de', 'la', 'triple', 'alianza', 'jam√°s', 'volvi√≥', 'Un', 'saludo', 'siempre', 'los', 'escucho', 'en', 'el', 'trabajo']","Tokens_without_stopwords":"['Los', 'paraguayos', 'llevamos', 'toda', 'vida', 'relaci√≥n', 'perros', 'gatos', 'argentinos', 'realidad', 'deber√≠amos', 'tener', 'relaci√≥n', 'Brasile√±os', 'jajaj', 'broma', 'Son', 'cosas', 'dejar', 'PASADO', 'ojal√°', 'vuelva', 'repetir', 'nunca', 'xq', 'si', 'vuelve', 'pasar', 'ahora', 'Paraguay', 'si', 'desaparecer√≠a', 'econom√≠a', 'guerra', 'triple', 'alianza', 'jam√°s', 'volvi√≥', 'Un', 'saludo', 'siempre', 'escucho', 'trabajo']","Tokens_without_stopwords_stemmed":"['los', 'paraguay', 'llev', 'tod', 'vid', 'relacion', 'perr', 'gat', 'argentin', 'realid', 'deb', 'ten', 'relacion', 'brasile√±', 'jajaj', 'brom', 'son', 'cos', 'dej', 'pas', 'ojal', 'vuelv', 'repet', 'nunc', 'xq', 'si', 'vuelv', 'pas', 'ahor', 'paraguay', 'si', 'desaparec', 'econom', 'guerr', 'tripl', 'alianz', 'jamas', 'volv', 'un', 'salud', 'siempr', 'escuch', 'trabaj']","Tokens_lemmatized":"['el', 'paraguayo', 'llevar', 'todo', 'el', 'vida', 'tener', 'uno', 'relaci√≥n', 'como', 'de', 'perro', 'y', 'gato', 'con', 'el', 'argentino', 'cuando', 'en', 'realidad', 'deber', 'tener', 'ese', 'relaci√≥n', 'con', 'el', 'Brasile√±os', 'jajaj', ',', 'ser', 'broma', 'üòâ', 'ser', 'cosa', 'que', 'tener', 'que', 'dejar', 'donde', 'estar', ',', 'que', 'ser', 'el', 'PASADO', 'y', 'que', 'ojal√°', 'no', '√©l', 'volver', 'a', 'repetir', 'nunca', 'xq', 'si', 'volver', 'a', 'pasar', 'ese', 'ahora', ',', 'paraguay', 'si', 'que', 'desaparecer', ',', 'el', 'econom√≠a', 'que', 'tener', 'antes', 'de', 'el', 'guerra', 'de', 'el', 'triple', 'alianza', 'jam√°s', 'volver', 'ü•≤', 'uno', 'saludo', 'ü´∂üèªsiempre', '√©l', 'escuchar', 'en', 'el', 'trabajo', '.']","Tokens_without_stopwords_lemmatized":"['el', 'paraguayo', 'llevar', 'todo', 'vida', 'relaci√≥n', 'perro', 'gato', 'argentino', 'realidad', 'deber', 'tener', 'relaci√≥n', 'Brasile√±os', 'jajaj', 'broma', 'ser', 'cosa', 'dejar', 'pasado', 'ojal√°', 'volver', 'repetir', 'nunca', 'xq', 'si', 'volver', 'pasar', 'ahora', 'paraguay', 'si', 'desaparecer', 'econom√≠a', 'guerra', 'triple', 'alianza', 'jam√°s', 'volver', 'uno', 'saludo', 'siempre', 'escuchar', 'trabajo']","_deepnote_index_column":5},{"id":"Ugwz1snOc8tFwlY2MbF4AaABAg","videoId":"RBnZPFhDcnk","textOriginal":"Que pedo con la gente que se queja de que digan groser√≠as, cuando es parte de la chispa que tiene el podcast. No cambien su escencia, la neta el programa est√° bien vrgs.","authorDisplayName":"@Vladi_Santos","likeCount":0,"publishedAt":"2024-07-07T21:41:24Z","category_id":5,"category_description":"Felicitaciones y agradecimientos","Tokens_full":"['Que', 'pedo', 'con', 'la', 'gente', 'que', 'se', 'queja', 'de', 'que', 'digan', 'groser√≠as', ',', 'cuando', 'es', 'parte', 'de', 'la', 'chispa', 'que', 'tiene', 'el', 'podcast', '.', 'No', 'cambien', 'su', 'escencia', ',', 'la', 'neta', 'el', 'programa', 'est√°', 'bien', 'vrgs', '.']","Tokens":"['Que', 'pedo', 'con', 'la', 'gente', 'que', 'se', 'queja', 'de', 'que', 'digan', 'groser√≠as', 'cuando', 'es', 'parte', 'de', 'la', 'chispa', 'que', 'tiene', 'el', 'podcast', 'No', 'cambien', 'su', 'escencia', 'la', 'neta', 'el', 'programa', 'est√°', 'bien', 'vrgs']","Tokens_without_stopwords":"['Que', 'pedo', 'gente', 'queja', 'digan', 'groser√≠as', 'parte', 'chispa', 'podcast', 'No', 'cambien', 'escencia', 'neta', 'programa', 'bien', 'vrgs']","Tokens_without_stopwords_stemmed":"['que', 'ped', 'gent', 'quej', 'dig', 'gros', 'part', 'chisp', 'podcast', 'no', 'cambi', 'escenci', 'net', 'program', 'bien', 'vrgs']","Tokens_lemmatized":"['que', 'pedar', 'con', 'el', 'gente', 'que', '√©l', 'quejar', 'de', 'que', 'decir', 'groser√≠a', ',', 'cuando', 'ser', 'parte', 'de', 'el', 'chispa', 'que', 'tener', 'el', 'podcast', '.', 'no', 'cambiar', 'su', 'escencia', ',', 'el', 'neta', 'el', 'programa', 'estar', 'bien', 'vrgs', '.']","Tokens_without_stopwords_lemmatized":"['que', 'pedar', 'gente', 'queja', 'decir', 'groser√≠a', 'parte', 'chispa', 'podcast', 'no', 'cambiar', 'escencia', 'neta', 'programa', 'bien', 'vrgs']","_deepnote_index_column":6},{"id":"UgxZXPGB2G4B2mjIRc94AaABAg","videoId":"CPCN1Lqzc2U","textOriginal":"Mi parte favorita de historia para tontos es cuando Iker y Tecate hacen el amor","authorDisplayName":"@josueestrada6170","likeCount":2,"publishedAt":"2024-06-26T05:25:01Z","category_id":6,"category_description":"Comentarios humor√≠sticos o memes","Tokens_full":"['Mi', 'parte', 'favorita', 'de', 'historia', 'para', 'tontos', 'es', 'cuando', 'Iker', 'y', 'Tecate', 'hacen', 'el', 'amor']","Tokens":"['Mi', 'parte', 'favorita', 'de', 'historia', 'para', 'tontos', 'es', 'cuando', 'Iker', 'y', 'Tecate', 'hacen', 'el', 'amor']","Tokens_without_stopwords":"['Mi', 'parte', 'favorita', 'historia', 'tontos', 'Iker', 'Tecate', 'hacen', 'amor']","Tokens_without_stopwords_stemmed":"['mi', 'part', 'favorit', 'histori', 'tont', 'iker', 'tecat', 'hac', 'amor']","Tokens_lemmatized":"['mi', 'parte', 'favorita', 'de', 'historia', 'para', 'tonto', 'ser', 'cuando', 'Iker', 'y', 'tecate', 'hacer', 'el', 'amor']","Tokens_without_stopwords_lemmatized":"['mi', 'parte', 'favorita', 'historia', 'tonto', 'Iker', 'tecate', 'hacer', 'amor']","_deepnote_index_column":7},{"id":"Ugyq_VGaO63x3zkeD454AaABAg","videoId":"yFHCam_Q9j4","textOriginal":"Weeeeeey","authorDisplayName":"@TheSuperdagon","likeCount":0,"publishedAt":"2024-04-25T00:01:49Z","category_id":6,"category_description":"Comentarios humor√≠sticos o memes","Tokens_full":"['Weeeeeey']","Tokens":"['Weeeeeey']","Tokens_without_stopwords":"['Weeeeeey']","Tokens_without_stopwords_stemmed":"['weeeeeey']","Tokens_lemmatized":"['weeeeeey']","Tokens_without_stopwords_lemmatized":"['weeeeeey']","_deepnote_index_column":8},{"id":"UgyUr5nbHHrU7jLb2Ml4AaABAg","videoId":"cQ9RLDhq6JY","textOriginal":"Hay alguna forma de hacer una donacion al podcast sin ser parte del patreon?. Este es uno de mis podcast favoritos","authorDisplayName":"@carlosrosas9850","likeCount":0,"publishedAt":"2024-04-11T00:33:00Z","category_id":7,"category_description":"Comentarios generales","Tokens_full":"['Hay', 'alguna', 'forma', 'de', 'hacer', 'una', 'donacion', 'al', 'podcast', 'sin', 'ser', 'parte', 'del', 'patreon', '?', '.', 'Este', 'es', 'uno', 'de', 'mis', 'podcast', 'favoritos']","Tokens":"['Hay', 'alguna', 'forma', 'de', 'hacer', 'una', 'donacion', 'al', 'podcast', 'sin', 'ser', 'parte', 'del', 'patreon', 'Este', 'es', 'uno', 'de', 'mis', 'podcast', 'favoritos']","Tokens_without_stopwords":"['Hay', 'alguna', 'forma', 'hacer', 'donacion', 'podcast', 'ser', 'parte', 'patreon', 'Este', 'podcast', 'favoritos']","Tokens_without_stopwords_stemmed":"['hay', 'algun', 'form', 'hac', 'donacion', 'podcast', 'ser', 'part', 'patreon', 'este', 'podcast', 'favorit']","Tokens_lemmatized":"['haber', 'alguno', 'forma', 'de', 'hacer', 'uno', 'donaci√≥n', 'a', 'el', 'podcast', 'sin', 'ser', 'parte', 'de', 'el', 'patre√≥n', '?', '.', 'este', 'ser', 'uno', 'de', 'mi', 'podcast', 'favorito']","Tokens_without_stopwords_lemmatized":"['haber', 'alguno', 'forma', 'hacer', 'donaci√≥n', 'podcast', 'ser', 'parte', 'patre√≥n', 'este', 'podcast', 'favorito']","_deepnote_index_column":9}]},"text/plain":"                             id      videoId  \\\n0    UgwtT3n4ZD6aX2c3NrF4AaABAg  FXxcNjTZ4qo   \n1    Ugzlbev9x6glV6YNpyN4AaABAg  CPCN1Lqzc2U   \n2    UgzIKrDRdcGk13aMtul4AaABAg  ShW6FY-vbmo   \n3    Ugz5Q5R5ls4pXTXSi294AaABAg  HxMXLWqe9HQ   \n4    UgwKlLDQnWVL_LOpE1d4AaABAg  wCm4FNSnDPs   \n..                          ...          ...   \n917  Ugx0EIHPfWZ1EV9dG4p4AaABAg  yFHCam_Q9j4   \n918  UgwSryWi0MPvIPVHDop4AaABAg  7Jo3NR5lgd8   \n919  Ugyn1Vg1Epb_ZoYbspB4AaABAg  FOFeh_vfcD8   \n920  Ugzu_MhkRKhOWF0SIk14AaABAg  yFHCam_Q9j4   \n921  UgyBc01slQ0uyw9NxAh4AaABAg  ShW6FY-vbmo   \n\n                                          textOriginal  \\\n0            No es un mito, Paraguay üáµüáæ si existe.....   \n1    Un podcast sobre la radio con DIANA URIBE, la ...   \n2    Cada vez que los escucho, muero de risa y me d...   \n3    Que buen capitulo, son geniales! El cristo del...   \n4    Gracias por el podcast de la semana, mis amore...   \n..                                                 ...   \n917  Me encant√≥ todo. Gran episodio üéâ! ¬°Felices 100...   \n918  Voy a investigar el conflicto de myanmar, ahor...   \n919  A ver a qu√© hora sacan otro podcast ? Ya pasar...   \n920  Me encant√≥ ver las fotos en el video de YouTub...   \n921     El editor no puso la naumaquima en el cuadro üò¢   \n\n               authorDisplayName  likeCount           publishedAt  \\\n0    @rosaelenarolonespinosa4666          1  2024-04-18T13:32:34Z   \n1                       @KarimVG          0  2024-06-26T05:30:57Z   \n2           @ceciliamendiola8646          2  2024-06-18T21:15:49Z   \n3    @anaberthalyochoaporras8262          0  2024-09-19T04:47:09Z   \n4                @LadySkywalkerW          0  2024-10-11T03:38:25Z   \n..                           ...        ...                   ...   \n917           @Cynthia_Gutierrez          0  2024-04-28T21:23:19Z   \n918          @antoniodejesus8241         15  2024-02-08T01:48:46Z   \n919           @YamilOrtega-rl9vb          1  2024-07-15T09:50:33Z   \n920              @veroaranda9618          0  2024-04-27T02:51:54Z   \n921              @Miguimin-fk8zc          0  2024-08-11T14:18:23Z   \n\n     category_id              category_description  \\\n0              7             Comentarios generales   \n1              5  Felicitaciones y agradecimientos   \n2              6  Comentarios humor√≠sticos o memes   \n3              5  Felicitaciones y agradecimientos   \n4              5  Felicitaciones y agradecimientos   \n..           ...                               ...   \n917            5  Felicitaciones y agradecimientos   \n918            3           Experiencias personales   \n919            1    Quejas o sugerencias de mejora   \n920            5  Felicitaciones y agradecimientos   \n921            4  Correcciones o datos adicionales   \n\n                                           Tokens_full  \\\n0    ['No', 'es', 'un', 'mito', ',', 'Paraguay', 's...   \n1    ['Un', 'podcast', 'sobre', 'la', 'radio', 'con...   \n2    ['Cada', 'vez', 'que', 'los', 'escucho', ',', ...   \n3    ['Que', 'buen', 'capitulo', ',', 'son', 'genia...   \n4    ['Gracias', 'por', 'el', 'podcast', 'de', 'la'...   \n..                                                 ...   \n917  ['Me', 'encant√≥', 'todo', '.', 'Gran', 'episod...   \n918  ['Voy', 'a', 'investigar', 'el', 'conflicto', ...   \n919  ['A', 'ver', 'a', 'qu√©', 'hora', 'sacan', 'otr...   \n920  ['Me', 'encant√≥', 'ver', 'las', 'fotos', 'en',...   \n921  ['El', 'editor', 'no', 'puso', 'la', 'naumaqui...   \n\n                                                Tokens  \\\n0    ['No', 'es', 'un', 'mito', 'Paraguay', 'si', '...   \n1    ['Un', 'podcast', 'sobre', 'la', 'radio', 'con...   \n2    ['Cada', 'vez', 'que', 'los', 'escucho', 'muer...   \n3    ['Que', 'buen', 'capitulo', 'son', 'geniales',...   \n4    ['Gracias', 'por', 'el', 'podcast', 'de', 'la'...   \n..                                                 ...   \n917  ['Me', 'encant√≥', 'todo', 'Gran', 'episodio', ...   \n918  ['Voy', 'a', 'investigar', 'el', 'conflicto', ...   \n919  ['A', 'ver', 'a', 'qu√©', 'hora', 'sacan', 'otr...   \n920  ['Me', 'encant√≥', 'ver', 'las', 'fotos', 'en',...   \n921  ['El', 'editor', 'no', 'puso', 'la', 'naumaqui...   \n\n                              Tokens_without_stopwords  \\\n0    ['No', 'mito', 'Paraguay', 'si', 'existe', '...']   \n1    ['Un', 'podcast', 'radio', 'DIANA', 'URIBE', '...   \n2    ['Cada', 'vez', 'escucho', 'muero', 'risa', 'd...   \n3    ['Que', 'buen', 'capitulo', 'geniales', 'El', ...   \n4    ['Gracias', 'podcast', 'semana', 'amores', 'Lo...   \n..                                                 ...   \n917  ['Me', 'encant√≥', 'Gran', 'episodio', 'Felices...   \n918  ['Voy', 'investigar', 'conflicto', 'myanmar', ...   \n919  ['A', 'ver', 'hora', 'sacan', 'podcast', 'Ya',...   \n920  ['Me', 'encant√≥', 'ver', 'fotos', 'video', 'Yo...   \n921   ['El', 'editor', 'puso', 'naumaquima', 'cuadro']   \n\n                      Tokens_without_stopwords_stemmed  \\\n0      ['no', 'mit', 'paraguay', 'si', 'exist', '...']   \n1    ['un', 'podcast', 'radi', 'dian', 'urib', 'mej...   \n2    ['cad', 'vez', 'escuch', 'muer', 'ris', 'dan',...   \n3    ['que', 'buen', 'capitul', 'genial', 'el', 'cr...   \n4    ['graci', 'podcast', 'seman', 'amor', 'los', '...   \n..                                                 ...   \n917  ['me', 'encant', 'gran', 'episodi', 'felic', '...   \n918  ['voy', 'investig', 'conflict', 'myanm', 'ahor...   \n919  ['a', 'ver', 'hor', 'sac', 'podcast', 'ya', 'p...   \n920   ['me', 'encant', 'ver', 'fot', 'vide', 'youtub']   \n921      ['el', 'editor', 'pus', 'naumaquim', 'cuadr']   \n\n                                     Tokens_lemmatized  \\\n0    ['no', 'ser', 'uno', 'mito', ',', 'paraguay', ...   \n1    ['uno', 'podcast', 'sobre', 'el', 'radio', 'co...   \n2    ['cada', 'vez', 'que', '√©l', 'escuchar', ',', ...   \n3    ['que', 'buen', 'capitulo', ',', 'ser', 'genia...   \n4    ['gracia', 'por', 'el', 'podcast', 'de', 'el',...   \n..                                                 ...   \n917  ['yo', 'encantar', 'todo', '.', 'Gran', 'episo...   \n918  ['ir', 'a', 'investigar', 'el', 'conflicto', '...   \n919  ['a', 'ver', 'a', 'qu√©', 'hora', 'sacar', 'otr...   \n920  ['yo', 'encantar', 'ver', 'el', 'foto', 'en', ...   \n921  ['el', 'editor', 'no', 'poner', 'el', 'naumaqu...   \n\n                   Tokens_without_stopwords_lemmatized  \n0    ['no', 'mito', 'paraguay', 'si', 'existir', '....  \n1    ['uno', 'podcast', 'radio', 'diana', 'URIBE', ...  \n2    ['cada', 'vez', 'escuchar', 'muero', 'risa', '...  \n3    ['que', 'buen', 'capitulo', 'genial', 'el', 'c...  \n4    ['gracia', 'podcast', 'semana', 'amor', 'el', ...  \n..                                                 ...  \n917  ['yo', 'encantar', 'gran', 'episodio', 'Felice...  \n918  ['ir', 'investigar', 'conflicto', 'myanmar', '...  \n919  ['a', 'ver', 'hora', 'sacar', 'podcast', 'ya',...  \n920  ['yo', 'encantar', 'ver', 'foto', 'v√≠deo', 'yo...  \n921  ['el', 'editor', 'poner', 'naumaquima', 'cuadro']  \n\n[922 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>videoId</th>\n      <th>textOriginal</th>\n      <th>authorDisplayName</th>\n      <th>likeCount</th>\n      <th>publishedAt</th>\n      <th>category_id</th>\n      <th>category_description</th>\n      <th>Tokens_full</th>\n      <th>Tokens</th>\n      <th>Tokens_without_stopwords</th>\n      <th>Tokens_without_stopwords_stemmed</th>\n      <th>Tokens_lemmatized</th>\n      <th>Tokens_without_stopwords_lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>UgwtT3n4ZD6aX2c3NrF4AaABAg</td>\n      <td>FXxcNjTZ4qo</td>\n      <td>No es un mito, Paraguay üáµüáæ si existe.....</td>\n      <td>@rosaelenarolonespinosa4666</td>\n      <td>1</td>\n      <td>2024-04-18T13:32:34Z</td>\n      <td>7</td>\n      <td>Comentarios generales</td>\n      <td>['No', 'es', 'un', 'mito', ',', 'Paraguay', 's...</td>\n      <td>['No', 'es', 'un', 'mito', 'Paraguay', 'si', '...</td>\n      <td>['No', 'mito', 'Paraguay', 'si', 'existe', '...']</td>\n      <td>['no', 'mit', 'paraguay', 'si', 'exist', '...']</td>\n      <td>['no', 'ser', 'uno', 'mito', ',', 'paraguay', ...</td>\n      <td>['no', 'mito', 'paraguay', 'si', 'existir', '....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ugzlbev9x6glV6YNpyN4AaABAg</td>\n      <td>CPCN1Lqzc2U</td>\n      <td>Un podcast sobre la radio con DIANA URIBE, la ...</td>\n      <td>@KarimVG</td>\n      <td>0</td>\n      <td>2024-06-26T05:30:57Z</td>\n      <td>5</td>\n      <td>Felicitaciones y agradecimientos</td>\n      <td>['Un', 'podcast', 'sobre', 'la', 'radio', 'con...</td>\n      <td>['Un', 'podcast', 'sobre', 'la', 'radio', 'con...</td>\n      <td>['Un', 'podcast', 'radio', 'DIANA', 'URIBE', '...</td>\n      <td>['un', 'podcast', 'radi', 'dian', 'urib', 'mej...</td>\n      <td>['uno', 'podcast', 'sobre', 'el', 'radio', 'co...</td>\n      <td>['uno', 'podcast', 'radio', 'diana', 'URIBE', ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>UgzIKrDRdcGk13aMtul4AaABAg</td>\n      <td>ShW6FY-vbmo</td>\n      <td>Cada vez que los escucho, muero de risa y me d...</td>\n      <td>@ceciliamendiola8646</td>\n      <td>2</td>\n      <td>2024-06-18T21:15:49Z</td>\n      <td>6</td>\n      <td>Comentarios humor√≠sticos o memes</td>\n      <td>['Cada', 'vez', 'que', 'los', 'escucho', ',', ...</td>\n      <td>['Cada', 'vez', 'que', 'los', 'escucho', 'muer...</td>\n      <td>['Cada', 'vez', 'escucho', 'muero', 'risa', 'd...</td>\n      <td>['cad', 'vez', 'escuch', 'muer', 'ris', 'dan',...</td>\n      <td>['cada', 'vez', 'que', '√©l', 'escuchar', ',', ...</td>\n      <td>['cada', 'vez', 'escuchar', 'muero', 'risa', '...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ugz5Q5R5ls4pXTXSi294AaABAg</td>\n      <td>HxMXLWqe9HQ</td>\n      <td>Que buen capitulo, son geniales! El cristo del...</td>\n      <td>@anaberthalyochoaporras8262</td>\n      <td>0</td>\n      <td>2024-09-19T04:47:09Z</td>\n      <td>5</td>\n      <td>Felicitaciones y agradecimientos</td>\n      <td>['Que', 'buen', 'capitulo', ',', 'son', 'genia...</td>\n      <td>['Que', 'buen', 'capitulo', 'son', 'geniales',...</td>\n      <td>['Que', 'buen', 'capitulo', 'geniales', 'El', ...</td>\n      <td>['que', 'buen', 'capitul', 'genial', 'el', 'cr...</td>\n      <td>['que', 'buen', 'capitulo', ',', 'ser', 'genia...</td>\n      <td>['que', 'buen', 'capitulo', 'genial', 'el', 'c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>UgwKlLDQnWVL_LOpE1d4AaABAg</td>\n      <td>wCm4FNSnDPs</td>\n      <td>Gracias por el podcast de la semana, mis amore...</td>\n      <td>@LadySkywalkerW</td>\n      <td>0</td>\n      <td>2024-10-11T03:38:25Z</td>\n      <td>5</td>\n      <td>Felicitaciones y agradecimientos</td>\n      <td>['Gracias', 'por', 'el', 'podcast', 'de', 'la'...</td>\n      <td>['Gracias', 'por', 'el', 'podcast', 'de', 'la'...</td>\n      <td>['Gracias', 'podcast', 'semana', 'amores', 'Lo...</td>\n      <td>['graci', 'podcast', 'seman', 'amor', 'los', '...</td>\n      <td>['gracia', 'por', 'el', 'podcast', 'de', 'el',...</td>\n      <td>['gracia', 'podcast', 'semana', 'amor', 'el', ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>Ugx0EIHPfWZ1EV9dG4p4AaABAg</td>\n      <td>yFHCam_Q9j4</td>\n      <td>Me encant√≥ todo. Gran episodio üéâ! ¬°Felices 100...</td>\n      <td>@Cynthia_Gutierrez</td>\n      <td>0</td>\n      <td>2024-04-28T21:23:19Z</td>\n      <td>5</td>\n      <td>Felicitaciones y agradecimientos</td>\n      <td>['Me', 'encant√≥', 'todo', '.', 'Gran', 'episod...</td>\n      <td>['Me', 'encant√≥', 'todo', 'Gran', 'episodio', ...</td>\n      <td>['Me', 'encant√≥', 'Gran', 'episodio', 'Felices...</td>\n      <td>['me', 'encant', 'gran', 'episodi', 'felic', '...</td>\n      <td>['yo', 'encantar', 'todo', '.', 'Gran', 'episo...</td>\n      <td>['yo', 'encantar', 'gran', 'episodio', 'Felice...</td>\n    </tr>\n    <tr>\n      <th>918</th>\n      <td>UgwSryWi0MPvIPVHDop4AaABAg</td>\n      <td>7Jo3NR5lgd8</td>\n      <td>Voy a investigar el conflicto de myanmar, ahor...</td>\n      <td>@antoniodejesus8241</td>\n      <td>15</td>\n      <td>2024-02-08T01:48:46Z</td>\n      <td>3</td>\n      <td>Experiencias personales</td>\n      <td>['Voy', 'a', 'investigar', 'el', 'conflicto', ...</td>\n      <td>['Voy', 'a', 'investigar', 'el', 'conflicto', ...</td>\n      <td>['Voy', 'investigar', 'conflicto', 'myanmar', ...</td>\n      <td>['voy', 'investig', 'conflict', 'myanm', 'ahor...</td>\n      <td>['ir', 'a', 'investigar', 'el', 'conflicto', '...</td>\n      <td>['ir', 'investigar', 'conflicto', 'myanmar', '...</td>\n    </tr>\n    <tr>\n      <th>919</th>\n      <td>Ugyn1Vg1Epb_ZoYbspB4AaABAg</td>\n      <td>FOFeh_vfcD8</td>\n      <td>A ver a qu√© hora sacan otro podcast ? Ya pasar...</td>\n      <td>@YamilOrtega-rl9vb</td>\n      <td>1</td>\n      <td>2024-07-15T09:50:33Z</td>\n      <td>1</td>\n      <td>Quejas o sugerencias de mejora</td>\n      <td>['A', 'ver', 'a', 'qu√©', 'hora', 'sacan', 'otr...</td>\n      <td>['A', 'ver', 'a', 'qu√©', 'hora', 'sacan', 'otr...</td>\n      <td>['A', 'ver', 'hora', 'sacan', 'podcast', 'Ya',...</td>\n      <td>['a', 'ver', 'hor', 'sac', 'podcast', 'ya', 'p...</td>\n      <td>['a', 'ver', 'a', 'qu√©', 'hora', 'sacar', 'otr...</td>\n      <td>['a', 'ver', 'hora', 'sacar', 'podcast', 'ya',...</td>\n    </tr>\n    <tr>\n      <th>920</th>\n      <td>Ugzu_MhkRKhOWF0SIk14AaABAg</td>\n      <td>yFHCam_Q9j4</td>\n      <td>Me encant√≥ ver las fotos en el video de YouTub...</td>\n      <td>@veroaranda9618</td>\n      <td>0</td>\n      <td>2024-04-27T02:51:54Z</td>\n      <td>5</td>\n      <td>Felicitaciones y agradecimientos</td>\n      <td>['Me', 'encant√≥', 'ver', 'las', 'fotos', 'en',...</td>\n      <td>['Me', 'encant√≥', 'ver', 'las', 'fotos', 'en',...</td>\n      <td>['Me', 'encant√≥', 'ver', 'fotos', 'video', 'Yo...</td>\n      <td>['me', 'encant', 'ver', 'fot', 'vide', 'youtub']</td>\n      <td>['yo', 'encantar', 'ver', 'el', 'foto', 'en', ...</td>\n      <td>['yo', 'encantar', 'ver', 'foto', 'v√≠deo', 'yo...</td>\n    </tr>\n    <tr>\n      <th>921</th>\n      <td>UgyBc01slQ0uyw9NxAh4AaABAg</td>\n      <td>ShW6FY-vbmo</td>\n      <td>El editor no puso la naumaquima en el cuadro üò¢</td>\n      <td>@Miguimin-fk8zc</td>\n      <td>0</td>\n      <td>2024-08-11T14:18:23Z</td>\n      <td>4</td>\n      <td>Correcciones o datos adicionales</td>\n      <td>['El', 'editor', 'no', 'puso', 'la', 'naumaqui...</td>\n      <td>['El', 'editor', 'no', 'puso', 'la', 'naumaqui...</td>\n      <td>['El', 'editor', 'puso', 'naumaquima', 'cuadro']</td>\n      <td>['el', 'editor', 'pus', 'naumaquim', 'cuadr']</td>\n      <td>['el', 'editor', 'no', 'poner', 'el', 'naumaqu...</td>\n      <td>['el', 'editor', 'poner', 'naumaquima', 'cuadro']</td>\n    </tr>\n  </tbody>\n</table>\n<p>922 rows √ó 14 columns</p>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/8a7497d7-44b6-4897-b87e-d37a44013902","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ff52d495","execution_start":1733898020119,"execution_millis":1,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"108c4714aba24bb7a5a101af575e12e6","deepnote_cell_type":"code"},"source":"df_train.columns","block_group":"48c26f28ff5e4a399da2291bb9f47973","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"Index(['id', 'videoId', 'textOriginal', 'authorDisplayName', 'likeCount',\n       'publishedAt', 'category_id', 'category_description', 'Tokens_full',\n       'Tokens', 'Tokens_without_stopwords',\n       'Tokens_without_stopwords_stemmed', 'Tokens_lemmatized',\n       'Tokens_without_stopwords_lemmatized'],\n      dtype='object')"},"metadata":{}}],"outputs_reference":"dbtable:cell_outputs/d12a66e5-114f-4475-99fc-69e2fff86434","content_dependencies":null},{"cell_type":"markdown","metadata":{"cell_id":"60c9aa386b454bbda08229893e3dbfb9","deepnote_cell_type":"markdown"},"source":"Create dataframe using only the `Tokens_without_stopwords_lemmatized` column from the `df_train`, `df_test` and `df_val` dataframe.","block_group":"0c490842d7a44fbf84eca14bf9056ebe"},{"cell_type":"code","metadata":{"source_hash":"9d32a950","execution_start":1733896909166,"execution_millis":0,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"33aaba62f8d54e67a852f14f30341d64","deepnote_cell_type":"code"},"source":"import ast\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport pandas as pd\n\n# Funci√≥n para evaluar de manera segura\ndef safe_eval(value):\n    \"\"\"\n    Convierte un valor de texto en su equivalente literal de Python, si es necesario.\n    Si ya es una lista, lo devuelve tal cual.\n    \"\"\"\n    try:\n        if isinstance(value, str):\n            return ast.literal_eval(value)\n        return value  # Si no es str, asume que ya es una lista\n    except (ValueError, SyntaxError):\n        print(f\"Error parsing: {value}\")\n        return []  # Devuelve una lista vac√≠a en caso de error\n\n# Funci√≥n para filtrar filas con listas no vac√≠as\ndef filter_non_empty_rows(df, column):\n    \"\"\"\n    Filtra un DataFrame para que solo contenga filas donde la columna especificada\n    tiene listas no vac√≠as.\n    \"\"\"\n    return df[df[column].apply(lambda x: len(x) > 0)]\n\n# Funci√≥n para generar reportes detallados\ndef generate_classification_report(y_true, y_pred, dataset_name):\n    \"\"\"\n    Genera y muestra un reporte detallado de m√©tricas de clasificaci√≥n.\n    \"\"\"\n    print(f\"\\n=== Classification Report for {dataset_name} ===\")\n    print(classification_report(y_true, y_pred))\n    print(f\"Confusion Matrix for {dataset_name}:\\n{confusion_matrix(y_true, y_pred)}\")\n\n# Funci√≥n principal para procesar una columna y generar reportes\ndef process_column_with_reports(df_train, df_test, df_val, column, classifier_class):\n    \"\"\"\n    Procesa una columna aplicando limpieza, filtrado y entrenamiento del modelo,\n    y genera reportes detallados de clasificaci√≥n para train, test y val.\n    \"\"\"\n    # Limpieza\n    df_train[column] = df_train[column].apply(safe_eval)\n    df_test[column] = df_test[column].apply(safe_eval)\n    df_val[column] = df_val[column].apply(safe_eval)\n    \n    # Filtrar filas con listas no vac√≠as\n    filtered_train_df = filter_non_empty_rows(df_train, column)\n    filtered_test_df = filter_non_empty_rows(df_test, column)\n    filtered_val_df = filter_non_empty_rows(df_val, column)\n    \n    # Dividir en X (features) e Y (target)\n    x_train = filtered_train_df[column]\n    y_train = filtered_train_df['category_id']\n    x_test = filtered_test_df[column]\n    y_test = filtered_test_df['category_id']\n    x_val = filtered_val_df[column]\n    y_val = filtered_val_df['category_id']\n    \n    # Crear y entrenar el clasificador\n    classifier = classifier_class()\n    classifier.fit(x_train, y_train)\n    \n    # Predecir para train, test y val\n    y_train_pred = classifier.predict(x_train)\n    y_test_pred = classifier.predict(x_test)\n    y_val_pred = classifier.predict(x_val)\n    \n    # Generar reportes detallados\n    generate_classification_report(y_train, y_train_pred, \"Train\")\n    generate_classification_report(y_test, y_test_pred, \"Test\")\n    generate_classification_report(y_val, y_val_pred, \"Validation\")\n    \n    return {\n        \"train_accuracy\": (y_train == y_train_pred).mean(),\n        \"test_accuracy\": (y_test == y_test_pred).mean(),\n        \"val_accuracy\": (y_val == y_val_pred).mean(),\n    }\n","block_group":"bf7c8f7e296a4b9eb1781ffb3310835f","execution_count":14,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"98a1a283","execution_start":1733898054390,"execution_millis":889,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"a7a30856eeef4eb1ad289db2d38f469a","deepnote_cell_type":"code"},"source":"cols = ['Tokens_full', 'Tokens', 'Tokens_without_stopwords', 'Tokens_without_stopwords_stemmed', 'Tokens_lemmatized',\n       'Tokens_without_stopwords_lemmatized']\n\nfor col in cols:\n    print(f\"Processing column: {col}\")\n    accuracies = process_column_with_reports(df_train, df_test, df_val, col, NaiveBayesClassifier)\n    print(f\"Accuracy Summary for {col}: {accuracies}\")\n","block_group":"26194603363043af9c4ba11363bc8eed","execution_count":24,"outputs":[{"name":"stdout","text":"Processing column: Tokens_full\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.92      0.28      0.43        82\n           2       0.87      0.89      0.88       151\n           3       1.00      0.13      0.23        62\n           4       0.52      0.99      0.68       115\n           5       0.58      0.98      0.73       191\n           6       0.93      0.76      0.84       206\n           7       1.00      0.17      0.28       109\n\n    accuracy                           0.70       916\n   macro avg       0.83      0.60      0.58       916\nweighted avg       0.81      0.70      0.66       916\n\nConfusion Matrix for Train:\n[[ 23   6   0  27  26   0   0]\n [  0 134   0   6  11   0   0]\n [  1   1   8  14  38   0   0]\n [  0   1   0 114   0   0   0]\n [  0   2   0   1 188   0   0]\n [  0   5   0  17  27 157   0]\n [  1   5   0  41  32  12  18]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.85      0.46      0.59        24\n           3       0.00      0.00      0.00         4\n           4       0.32      0.83      0.47        12\n           5       0.50      0.96      0.66        23\n           6       0.50      0.12      0.19        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.49        92\n   macro avg       0.31      0.34      0.27        92\nweighted avg       0.48      0.49      0.42        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  3  3  0  0]\n [ 0 11  0  4  8  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  0  0 10  2  0  0]\n [ 0  0  0  1 22  0  0]\n [ 0  0  0 11  4  2  0]\n [ 0  1  0  2  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.33      0.50      0.40         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.27        11\n   macro avg       0.22      0.19      0.20        11\nweighted avg       0.33      0.27      0.29        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 2 0 0]\n [0 0 0 1 1 0]]\nAccuracy Summary for Tokens_full: {'train_accuracy': 0.7008733624454149, 'test_accuracy': 0.4891304347826087, 'val_accuracy': 0.2727272727272727}\nProcessing column: Tokens\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.94      0.39      0.55        82\n           2       0.89      0.89      0.89       151\n           3       1.00      0.18      0.30        62\n           4       0.58      0.99      0.73       115\n           5       0.58      0.98      0.73       191\n           6       0.94      0.81      0.87       206\n           7       1.00      0.21      0.35       109\n\n    accuracy                           0.73       916\n   macro avg       0.85      0.64      0.63       916\nweighted avg       0.82      0.73      0.70       916\n\nConfusion Matrix for Train:\n[[ 32   6   0  18  26   0   0]\n [  0 134   0   5  12   0   0]\n [  1   0  11  11  39   0   0]\n [  0   1   0 114   0   0   0]\n [  0   2   0   1 188   0   0]\n [  0   4   0  13  23 166   0]\n [  1   4   0  36  35  10  23]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.79      0.46      0.58        24\n           3       0.00      0.00      0.00         4\n           4       0.31      0.75      0.44        12\n           5       0.50      0.96      0.66        23\n           6       0.40      0.12      0.18        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.48        92\n   macro avg       0.29      0.33      0.27        92\nweighted avg       0.44      0.48      0.41        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  3  3  0  0]\n [ 0 11  0  3  9  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  1  0  9  2  0  0]\n [ 0  0  0  1 22  0  0]\n [ 0  0  0 11  4  2  0]\n [ 0  1  0  2  0  2  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.33      0.50      0.40         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.27        11\n   macro avg       0.22      0.19      0.20        11\nweighted avg       0.33      0.27      0.29        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 2 0 0]\n [0 0 0 1 1 0]]\nAccuracy Summary for Tokens: {'train_accuracy': 0.7292576419213974, 'test_accuracy': 0.4782608695652174, 'val_accuracy': 0.2727272727272727}\nProcessing column: Tokens_without_stopwords\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.98      0.76      0.86        82\n           2       0.90      0.97      0.94       151\n           3       1.00      0.71      0.83        62\n           4       0.94      0.96      0.95       115\n           5       0.74      0.98      0.84       191\n           6       0.92      0.95      0.94       206\n           7       1.00      0.59      0.74       109\n\n    accuracy                           0.88       916\n   macro avg       0.93      0.84      0.87       916\nweighted avg       0.90      0.88      0.88       916\n\nConfusion Matrix for Train:\n[[ 62   5   0   0  14   1   0]\n [  0 147   0   0   4   0   0]\n [  1   0  44   1  16   0   0]\n [  0   2   0 110   2   1   0]\n [  0   2   0   0 188   1   0]\n [  0   1   0   0  10 195   0]\n [  0   6   0   6  20  13  64]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.78      0.58      0.67        24\n           3       0.00      0.00      0.00         4\n           4       0.36      0.33      0.35        12\n           5       0.45      0.91      0.60        23\n           6       0.43      0.35      0.39        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.50        92\n   macro avg       0.43      0.33      0.32        92\nweighted avg       0.52      0.50      0.46        92\n\nConfusion Matrix for Test:\n[[ 1  1  0  1  4  0  0]\n [ 0 14  0  0  9  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  2  0  4  3  3  0]\n [ 0  0  0  0 21  2  0]\n [ 0  0  1  4  6  6  0]\n [ 0  1  0  2  0  2  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         2\n           5       0.00      0.00      0.00         1\n           6       0.20      0.50      0.29         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.36        11\n   macro avg       0.37      0.28      0.29        11\nweighted avg       0.49      0.36      0.39        11\n\nConfusion Matrix for Validation:\n[[2 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 1]]\nAccuracy Summary for Tokens_without_stopwords: {'train_accuracy': 0.8842794759825328, 'test_accuracy': 0.5, 'val_accuracy': 0.36363636363636365}\nProcessing column: Tokens_without_stopwords_stemmed\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.97      0.48      0.64        82\n           2       0.82      0.86      0.84       151\n           3       1.00      0.39      0.56        62\n           4       0.81      0.91      0.86       115\n           5       0.57      0.98      0.72       191\n           6       0.87      0.85      0.86       206\n           7       1.00      0.33      0.50       109\n\n    accuracy                           0.76       916\n   macro avg       0.86      0.69      0.71       916\nweighted avg       0.83      0.76      0.75       916\n\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nConfusion Matrix for Train:\n[[ 39   8   0   3  30   2   0]\n [  0 130   0   2  16   3   0]\n [  1   0  24   4  31   2   0]\n [  0   4   0 105   5   1   0]\n [  0   3   0   0 187   1   0]\n [  0   5   0   4  21 176   0]\n [  0   9   0  11  36  17  36]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.68      0.54      0.60        24\n           3       0.00      0.00      0.00         4\n           4       0.47      0.58      0.52        12\n           5       0.48      0.96      0.64        23\n           6       0.64      0.41      0.50        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.54        92\n   macro avg       0.47      0.38      0.36        92\nweighted avg       0.55      0.54      0.50        92\n\nConfusion Matrix for Test:\n[[ 1  1  0  0  5  0  0]\n [ 0 13  0  1  8  2  0]\n [ 0  0  0  0  4  0  0]\n [ 0  2  0  7  2  1  0]\n [ 0  0  0  1 22  0  0]\n [ 0  1  0  5  4  7  0]\n [ 0  2  0  1  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       0.67      0.67      0.67         3\n           3       0.00      0.00      0.00         1\n           4       0.50      0.50      0.50         2\n           5       0.00      0.00      0.00         1\n           6       0.50      0.50      0.50         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.45        11\n   macro avg       0.44      0.36      0.39        11\nweighted avg       0.55      0.45      0.48        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [1 0 0 0 0 1]]\nAccuracy Summary for Tokens_without_stopwords_stemmed: {'train_accuracy': 0.7609170305676856, 'test_accuracy': 0.5434782608695652, 'val_accuracy': 0.45454545454545453}\nProcessing column: Tokens_lemmatized\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.10      0.18        82\n           2       0.88      0.72      0.79       151\n           3       1.00      0.06      0.12        62\n           4       0.51      0.98      0.67       115\n           5       0.48      1.00      0.65       194\n           6       0.95      0.69      0.80       206\n           7       1.00      0.12      0.22       112\n\n    accuracy                           0.63       922\n   macro avg       0.83      0.53      0.49       922\nweighted avg       0.80      0.63      0.58       922\n\nConfusion Matrix for Train:\n[[  8   5   0  24  44   1   0]\n [  0 109   0  11  31   0   0]\n [  0   0   4  13  45   0   0]\n [  0   1   0 113   1   0   0]\n [  0   0   0   0 194   0   0]\n [  0   2   0  20  42 142   0]\n [  0   7   0  41  44   6  14]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.82      0.38      0.51        24\n           3       0.00      0.00      0.00         4\n           4       0.36      0.75      0.49        12\n           5       0.40      0.91      0.56        23\n           6       0.50      0.12      0.19        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.45        92\n   macro avg       0.30      0.31      0.25        92\nweighted avg       0.45      0.45      0.37        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  1  5  0  0]\n [ 0  9  0  3 11  1  0]\n [ 0  0  0  1  3  0  0]\n [ 0  0  0  9  3  0  0]\n [ 0  0  0  2 21  0  0]\n [ 0  0  0  9  6  2  0]\n [ 0  1  0  0  3  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.40      1.00      0.57         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.36        11\n   macro avg       0.23      0.28      0.23        11\nweighted avg       0.35      0.36      0.32        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 2 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 1 0 0]\n [0 0 0 2 0 0]]\nAccuracy Summary for Tokens_lemmatized: {'train_accuracy': 0.6334056399132321, 'test_accuracy': 0.44565217391304346, 'val_accuracy': 0.36363636363636365}\nProcessing column: Tokens_without_stopwords_lemmatized\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.51      0.68        82\n           2       0.85      0.85      0.85       151\n           3       1.00      0.44      0.61        62\n           4       0.85      0.95      0.90       115\n           5       0.60      0.98      0.74       191\n           6       0.87      0.91      0.89       206\n           7       1.00      0.36      0.53       109\n\n    accuracy                           0.79       916\n   macro avg       0.88      0.71      0.74       916\nweighted avg       0.84      0.79      0.77       916\n\nConfusion Matrix for Train:\n[[ 42   6   0   2  29   3   0]\n [  0 128   0   3  19   1   0]\n [  0   0  27   4  28   3   0]\n [  0   2   0 109   3   1   0]\n [  0   2   0   0 188   1   0]\n [  0   4   0   1  14 187   0]\n [  0   8   0   9  34  19  39]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.65      0.46      0.54        24\n           3       0.00      0.00      0.00         4\n           4       0.44      0.58      0.50        12\n           5       0.43      0.87      0.58        23\n           6       0.67      0.47      0.55        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.51        92\n   macro avg       0.46      0.36      0.35        92\nweighted avg       0.53      0.51      0.47        92\n\nConfusion Matrix for Test:\n[[ 1  1  0  0  5  0  0]\n [ 0 11  0  1 10  2  0]\n [ 0  0  0  0  4  0  0]\n [ 0  3  0  7  1  1  0]\n [ 0  1  0  2 20  0  0]\n [ 0  0  0  4  5  8  0]\n [ 0  1  0  2  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         2\n           5       0.00      0.00      0.00         1\n           6       0.33      0.50      0.40         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.36        11\n   macro avg       0.39      0.28      0.31        11\nweighted avg       0.52      0.36      0.41        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [0 0 0 1 0 1]]\nAccuracy Summary for Tokens_without_stopwords_lemmatized: {'train_accuracy': 0.7860262008733624, 'test_accuracy': 0.5108695652173914, 'val_accuracy': 0.36363636363636365}\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/87666195-fd83-4b7f-b088-8d72c3d8debe","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9ce8fb41","execution_start":1733897501841,"execution_millis":53,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"416ee5e5a6004a43bbb9327cb9d2ebe9","deepnote_cell_type":"code"},"source":"cols = ['Tokens_without_stopwords']\n\nfor col in cols:\n    print(f\"Processing column: {col}\")\n    accuracies = process_column_with_reports(df_train_2, df_test_2, df_val_2, col, NaiveBayesClassifier)\n    print(f\"Accuracy Summary for {col}: {accuracies}\")\n","block_group":"f39121face5b42f390d587f38bb67c99","execution_count":21,"outputs":[{"name":"stdout","text":"Processing column: Tokens_without_stopwords\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.87      0.41      0.55        81\n           2       0.63      0.71      0.67       145\n           3       0.90      0.31      0.46        61\n           4       0.64      0.67      0.65       111\n           5       0.50      0.95      0.65       195\n           6       0.77      0.55      0.64       179\n           7       1.00      0.19      0.32        80\n\n    accuracy                           0.62       852\n   macro avg       0.76      0.54      0.56       852\nweighted avg       0.71      0.62      0.60       852\n\nConfusion Matrix for Train:\n[[ 33  15   0   3  26   4   0]\n [  0 103   1   7  31   3   0]\n [  2   4  19   4  30   2   0]\n [  0   9   0  74  21   7   0]\n [  0   7   0   1 185   2   0]\n [  1  14   0  12  54  98   0]\n [  2  12   1  14  24  12  15]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         5\n           2       0.47      0.45      0.46        20\n           3       0.50      0.20      0.29         5\n           4       0.27      0.25      0.26        12\n           5       0.28      0.92      0.43        13\n           6       0.50      0.16      0.24        19\n           7       1.00      0.12      0.22         8\n\n    accuracy                           0.35        82\n   macro avg       0.43      0.30      0.27        82\nweighted avg       0.44      0.35      0.31        82\n\nConfusion Matrix for Test:\n[[ 0  2  0  0  3  0  0]\n [ 0  9  1  3  7  0  0]\n [ 0  0  1  0  3  1  0]\n [ 0  3  0  3  5  1  0]\n [ 0  1  0  0 12  0  0]\n [ 0  3  0  3 10  3  0]\n [ 0  1  0  2  3  1  1]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      1.00      1.00         4\n           4       0.00      0.00      0.00         1\n           5       0.00      0.00      0.00         0\n           6       0.00      0.00      0.00         3\n           7       0.00      0.00      0.00         1\n\n    accuracy                           0.44         9\n   macro avg       0.20      0.20      0.20         9\nweighted avg       0.44      0.44      0.44         9\n\nConfusion Matrix for Validation:\n[[4 0 0 0 0]\n [0 0 1 0 0]\n [0 0 0 0 0]\n [0 0 3 0 0]\n [0 0 1 0 0]]\nAccuracy Summary for Tokens_without_stopwords: {'train_accuracy': 0.6185446009389671, 'test_accuracy': 0.35365853658536583, 'val_accuracy': 0.4444444444444444}\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"outputs_reference":"s3:deepnote-cell-outputs-production/74c99f82-dfec-493a-8b05-ae2b5499de2b","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"3509d8cb","execution_start":1733898209140,"execution_millis":0,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"2ad4eea0256b4721bb8c4f8f6b5b3492","deepnote_cell_type":"code"},"source":"def collect_accuracies(df_train, df_test, df_val, cols, classifier_class):\n    \"\"\"\n    Procesa cada columna, entrena y eval√∫a el modelo, y almacena las precisiones en un DataFrame.\n    \n    Parameters:\n        df_train, df_test, df_val: DataFrames de entrenamiento, prueba y validaci√≥n.\n        cols: Lista de columnas a procesar.\n        classifier_class: Clase del modelo de clasificaci√≥n a usar.\n        \n    Returns:\n        results_df: DataFrame con las precisiones para cada columna.\n    \"\"\"\n    results = []\n\n    for col in cols:\n        print(f\"Processing column: {col}\")\n        accuracies = process_column_with_reports(df_train, df_test, df_val, col, classifier_class)\n        results.append({\n            \"Column\": col,\n            \"Train Accuracy\": accuracies[\"train_accuracy\"],\n            \"Test Accuracy\": accuracies[\"test_accuracy\"],\n            \"Validation Accuracy\": accuracies[\"val_accuracy\"]\n        })\n    \n    results_df = pd.DataFrame(results)\n    return results_df","block_group":"9a9e3cab15544049b77e7ddc152f979d","execution_count":26,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"889ab31a","execution_start":1733898230762,"execution_millis":638,"execution_context_id":"41cfbd2e-b280-45a1-b4a8-14620261accf","cell_id":"47dbb6a168794558ae6e9c37412be806","deepnote_cell_type":"code"},"source":"results_df = collect_accuracies(df_train, df_test, df_val, cols, NaiveBayesClassifier)\nresults_df","block_group":"ee1d38ffc0f249bc888f6ced75d13327","execution_count":27,"outputs":[{"name":"stdout","text":"Processing column: Tokens_full\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.92      0.28      0.43        82\n           2       0.87      0.89      0.88       151\n           3       1.00      0.13      0.23        62\n           4       0.52      0.99      0.68       115\n           5       0.58      0.98      0.73       191\n           6       0.93      0.76      0.84       206\n           7       1.00      0.17      0.28       109\n\n    accuracy                           0.70       916\n   macro avg       0.83      0.60      0.58       916\nweighted avg       0.81      0.70      0.66       916\n\nConfusion Matrix for Train:\n[[ 23   6   0  27  26   0   0]\n [  0 134   0   6  11   0   0]\n [  1   1   8  14  38   0   0]\n [  0   1   0 114   0   0   0]\n [  0   2   0   1 188   0   0]\n [  0   5   0  17  27 157   0]\n [  1   5   0  41  32  12  18]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.85      0.46      0.59        24\n           3       0.00      0.00      0.00         4\n           4       0.32      0.83      0.47        12\n           5       0.50      0.96      0.66        23\n           6       0.50      0.12      0.19        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.49        92\n   macro avg       0.31      0.34      0.27        92\nweighted avg       0.48      0.49      0.42        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  3  3  0  0]\n [ 0 11  0  4  8  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  0  0 10  2  0  0]\n [ 0  0  0  1 22  0  0]\n [ 0  0  0 11  4  2  0]\n [ 0  1  0  2  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.33      0.50      0.40         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.27        11\n   macro avg       0.22      0.19      0.20        11\nweighted avg       0.33      0.27      0.29        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 2 0 0]\n [0 0 0 1 1 0]]\nProcessing column: Tokens\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.94      0.39      0.55        82\n           2       0.89      0.89      0.89       151\n           3       1.00      0.18      0.30        62\n           4       0.58      0.99      0.73       115\n           5       0.58      0.98      0.73       191\n           6       0.94      0.81      0.87       206\n           7       1.00      0.21      0.35       109\n\n    accuracy                           0.73       916\n   macro avg       0.85      0.64      0.63       916\nweighted avg       0.82      0.73      0.70       916\n\nConfusion Matrix for Train:\n[[ 32   6   0  18  26   0   0]\n [  0 134   0   5  12   0   0]\n [  1   0  11  11  39   0   0]\n [  0   1   0 114   0   0   0]\n [  0   2   0   1 188   0   0]\n [  0   4   0  13  23 166   0]\n [  1   4   0  36  35  10  23]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.79      0.46      0.58        24\n           3       0.00      0.00      0.00         4\n           4       0.31      0.75      0.44        12\n           5       0.50      0.96      0.66        23\n           6       0.40      0.12      0.18        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.48        92\n   macro avg       0.29      0.33      0.27        92\nweighted avg       0.44      0.48      0.41        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  3  3  0  0]\n [ 0 11  0  3  9  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  1  0  9  2  0  0]\n [ 0  0  0  1 22  0  0]\n [ 0  0  0 11  4  2  0]\n [ 0  1  0  2  0  2  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.33      0.50      0.40         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.27        11\n   macro avg       0.22      0.19      0.20        11\nweighted avg       0.33      0.27      0.29        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 2 0 0]\n [0 0 0 1 1 0]]\nProcessing column: Tokens_without_stopwords\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.98      0.76      0.86        82\n           2       0.90      0.97      0.94       151\n           3       1.00      0.71      0.83        62\n           4       0.94      0.96      0.95       115\n           5       0.74      0.98      0.84       191\n           6       0.92      0.95      0.94       206\n           7       1.00      0.59      0.74       109\n\n    accuracy                           0.88       916\n   macro avg       0.93      0.84      0.87       916\nweighted avg       0.90      0.88      0.88       916\n\nConfusion Matrix for Train:\n[[ 62   5   0   0  14   1   0]\n [  0 147   0   0   4   0   0]\n [  1   0  44   1  16   0   0]\n [  0   2   0 110   2   1   0]\n [  0   2   0   0 188   1   0]\n [  0   1   0   0  10 195   0]\n [  0   6   0   6  20  13  64]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.78      0.58      0.67        24\n           3       0.00      0.00      0.00         4\n           4       0.36      0.33      0.35        12\n           5       0.45      0.91      0.60        23\n           6       0.43      0.35      0.39        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.50        92\n   macro avg       0.43      0.33      0.32        92\nweighted avg       0.52      0.50      0.46        92\n\nConfusion Matrix for Test:\n[[ 1  1  0  1  4  0  0]\n [ 0 14  0  0  9  1  0]\n [ 0  0  0  0  4  0  0]\n [ 0  2  0  4  3  3  0]\n [ 0  0  0  0 21  2  0]\n [ 0  0  1  4  6  6  0]\n [ 0  1  0  2  0  2  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         2\n           5       0.00      0.00      0.00         1\n           6       0.20      0.50      0.29         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.36        11\n   macro avg       0.37      0.28      0.29        11\nweighted avg       0.49      0.36      0.39        11\n\nConfusion Matrix for Validation:\n[[2 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 1]]\nProcessing column: Tokens_without_stopwords_stemmed\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       0.97      0.48      0.64        82\n           2       0.82      0.86      0.84       151\n           3       1.00      0.39      0.56        62\n           4       0.81      0.91      0.86       115\n           5       0.57      0.98      0.72       191\n           6       0.87      0.85      0.86       206\n           7       1.00      0.33      0.50       109\n\n    accuracy                           0.76       916\n   macro avg       0.86      0.69      0.71       916\nweighted avg       0.83      0.76      0.75       916\n\nConfusion Matrix for Train:\n[[ 39   8   0   3  30   2   0]\n [  0 130   0   2  16   3   0]\n [  1   0  24   4  31   2   0]\n [  0   4   0 105   5   1   0]\n [  0   3   0   0 187   1   0]\n [  0   5   0   4  21 176   0]\n [  0   9   0  11  36  17  36]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.68      0.54      0.60        24\n           3       0.00      0.00      0.00         4\n           4       0.47      0.58      0.52        12\n           5       0.48      0.96      0.64        23\n           6       0.64      0.41      0.50        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.54        92\n   macro avg       0.47      0.38      0.36        92\nweighted avg       0.55      0.54      0.50        92\n\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nConfusion Matrix for Test:\n[[ 1  1  0  0  5  0  0]\n [ 0 13  0  1  8  2  0]\n [ 0  0  0  0  4  0  0]\n [ 0  2  0  7  2  1  0]\n [ 0  0  0  1 22  0  0]\n [ 0  1  0  5  4  7  0]\n [ 0  2  0  1  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       0.67      0.67      0.67         3\n           3       0.00      0.00      0.00         1\n           4       0.50      0.50      0.50         2\n           5       0.00      0.00      0.00         1\n           6       0.50      0.50      0.50         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.45        11\n   macro avg       0.44      0.36      0.39        11\nweighted avg       0.55      0.45      0.48        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [1 0 0 0 0 1]]\nProcessing column: Tokens_lemmatized\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.10      0.18        82\n           2       0.88      0.72      0.79       151\n           3       1.00      0.06      0.12        62\n           4       0.51      0.98      0.67       115\n           5       0.48      1.00      0.65       194\n           6       0.95      0.69      0.80       206\n           7       1.00      0.12      0.22       112\n\n    accuracy                           0.63       922\n   macro avg       0.83      0.53      0.49       922\nweighted avg       0.80      0.63      0.58       922\n\nConfusion Matrix for Train:\n[[  8   5   0  24  44   1   0]\n [  0 109   0  11  31   0   0]\n [  0   0   4  13  45   0   0]\n [  0   1   0 113   1   0   0]\n [  0   0   0   0 194   0   0]\n [  0   2   0  20  42 142   0]\n [  0   7   0  41  44   6  14]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       0.00      0.00      0.00         7\n           2       0.82      0.38      0.51        24\n           3       0.00      0.00      0.00         4\n           4       0.36      0.75      0.49        12\n           5       0.40      0.91      0.56        23\n           6       0.50      0.12      0.19        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.45        92\n   macro avg       0.30      0.31      0.25        92\nweighted avg       0.45      0.45      0.37        92\n\nConfusion Matrix for Test:\n[[ 0  1  0  1  5  0  0]\n [ 0  9  0  3 11  1  0]\n [ 0  0  0  1  3  0  0]\n [ 0  0  0  9  3  0  0]\n [ 0  0  0  2 21  0  0]\n [ 0  0  0  9  6  2  0]\n [ 0  1  0  0  3  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.40      1.00      0.57         2\n           5       0.00      0.00      0.00         1\n           6       0.00      0.00      0.00         2\n           7       0.00      0.00      0.00         2\n\n    accuracy                           0.36        11\n   macro avg       0.23      0.28      0.23        11\nweighted avg       0.35      0.36      0.32        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 2 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 1 0 0]\n [0 0 0 2 0 0]]\nProcessing column: Tokens_without_stopwords_lemmatized\n\n=== Classification Report for Train ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.51      0.68        82\n           2       0.85      0.85      0.85       151\n           3       1.00      0.44      0.61        62\n           4       0.85      0.95      0.90       115\n           5       0.60      0.98      0.74       191\n           6       0.87      0.91      0.89       206\n           7       1.00      0.36      0.53       109\n\n    accuracy                           0.79       916\n   macro avg       0.88      0.71      0.74       916\nweighted avg       0.84      0.79      0.77       916\n\nConfusion Matrix for Train:\n[[ 42   6   0   2  29   3   0]\n [  0 128   0   3  19   1   0]\n [  0   0  27   4  28   3   0]\n [  0   2   0 109   3   1   0]\n [  0   2   0   0 188   1   0]\n [  0   4   0   1  14 187   0]\n [  0   8   0   9  34  19  39]]\n\n=== Classification Report for Test ===\n              precision    recall  f1-score   support\n\n           1       1.00      0.14      0.25         7\n           2       0.65      0.46      0.54        24\n           3       0.00      0.00      0.00         4\n           4       0.44      0.58      0.50        12\n           5       0.43      0.87      0.58        23\n           6       0.67      0.47      0.55        17\n           7       0.00      0.00      0.00         5\n\n    accuracy                           0.51        92\n   macro avg       0.46      0.36      0.35        92\nweighted avg       0.53      0.51      0.47        92\n\nConfusion Matrix for Test:\n[[ 1  1  0  0  5  0  0]\n [ 0 11  0  1 10  2  0]\n [ 0  0  0  0  4  0  0]\n [ 0  3  0  7  1  1  0]\n [ 0  1  0  2 20  0  0]\n [ 0  0  0  4  5  8  0]\n [ 0  1  0  2  1  1  0]]\n\n=== Classification Report for Validation ===\n              precision    recall  f1-score   support\n\n           2       1.00      0.67      0.80         3\n           3       0.00      0.00      0.00         1\n           4       0.00      0.00      0.00         2\n           5       0.00      0.00      0.00         1\n           6       0.33      0.50      0.40         2\n           7       1.00      0.50      0.67         2\n\n    accuracy                           0.36        11\n   macro avg       0.39      0.28      0.31        11\nweighted avg       0.52      0.36      0.41        11\n\nConfusion Matrix for Validation:\n[[2 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 1 0]\n [0 0 0 0 1 0]\n [0 0 0 1 1 0]\n [0 0 0 1 0 1]]\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/root/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"output_type":"execute_result","execution_count":27,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":4,"row_count":6,"columns":[{"name":"Column","dtype":"object","stats":{"unique_count":6,"nan_count":0,"categories":[{"name":"Tokens_full","count":1},{"name":"Tokens","count":1},{"name":"4 others","count":4}]}},{"name":"Train Accuracy","dtype":"float64","stats":{"unique_count":6,"nan_count":0,"min":"0.6334056399132321","max":"0.8842794759825328","histogram":[{"bin_start":0.6334056399132321,"bin_end":0.6584930235201621,"count":1},{"bin_start":0.6584930235201621,"bin_end":0.6835804071270922,"count":0},{"bin_start":0.6835804071270922,"bin_end":0.7086677907340223,"count":1},{"bin_start":0.7086677907340223,"bin_end":0.7337551743409524,"count":1},{"bin_start":0.7337551743409524,"bin_end":0.7588425579478824,"count":0},{"bin_start":0.7588425579478824,"bin_end":0.7839299415548124,"count":1},{"bin_start":0.7839299415548124,"bin_end":0.8090173251617425,"count":1},{"bin_start":0.8090173251617425,"bin_end":0.8341047087686726,"count":0},{"bin_start":0.8341047087686726,"bin_end":0.8591920923756027,"count":0},{"bin_start":0.8591920923756027,"bin_end":0.8842794759825328,"count":1}]}},{"name":"Test Accuracy","dtype":"float64","stats":{"unique_count":6,"nan_count":0,"min":"0.44565217391304346","max":"0.5434782608695652","histogram":[{"bin_start":0.44565217391304346,"bin_end":0.45543478260869563,"count":1},{"bin_start":0.45543478260869563,"bin_end":0.4652173913043478,"count":0},{"bin_start":0.4652173913043478,"bin_end":0.475,"count":0},{"bin_start":0.475,"bin_end":0.48478260869565215,"count":1},{"bin_start":0.48478260869565215,"bin_end":0.4945652173913043,"count":1},{"bin_start":0.4945652173913043,"bin_end":0.5043478260869565,"count":1},{"bin_start":0.5043478260869565,"bin_end":0.5141304347826087,"count":1},{"bin_start":0.5141304347826087,"bin_end":0.5239130434782608,"count":0},{"bin_start":0.5239130434782608,"bin_end":0.533695652173913,"count":0},{"bin_start":0.533695652173913,"bin_end":0.5434782608695652,"count":1}]}},{"name":"Validation Accuracy","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":"0.2727272727272727","max":"0.45454545454545453","histogram":[{"bin_start":0.2727272727272727,"bin_end":0.2909090909090909,"count":2},{"bin_start":0.2909090909090909,"bin_end":0.3090909090909091,"count":0},{"bin_start":0.3090909090909091,"bin_end":0.32727272727272727,"count":0},{"bin_start":0.32727272727272727,"bin_end":0.34545454545454546,"count":0},{"bin_start":0.34545454545454546,"bin_end":0.36363636363636365,"count":0},{"bin_start":0.36363636363636365,"bin_end":0.3818181818181818,"count":3},{"bin_start":0.3818181818181818,"bin_end":0.39999999999999997,"count":0},{"bin_start":0.39999999999999997,"bin_end":0.41818181818181815,"count":0},{"bin_start":0.41818181818181815,"bin_end":0.43636363636363634,"count":0},{"bin_start":0.43636363636363634,"bin_end":0.45454545454545453,"count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"Column":"Tokens_full","Train Accuracy":0.7008733624454149,"Test Accuracy":0.4891304347826087,"Validation Accuracy":0.2727272727272727,"_deepnote_index_column":0},{"Column":"Tokens","Train Accuracy":0.7292576419213974,"Test Accuracy":0.4782608695652174,"Validation Accuracy":0.2727272727272727,"_deepnote_index_column":1},{"Column":"Tokens_without_stopwords","Train Accuracy":0.8842794759825328,"Test Accuracy":0.5,"Validation Accuracy":0.36363636363636365,"_deepnote_index_column":2},{"Column":"Tokens_without_stopwords_stemmed","Train Accuracy":0.7609170305676856,"Test Accuracy":0.5434782608695652,"Validation Accuracy":0.45454545454545453,"_deepnote_index_column":3},{"Column":"Tokens_lemmatized","Train Accuracy":0.6334056399132321,"Test Accuracy":0.44565217391304346,"Validation Accuracy":0.36363636363636365,"_deepnote_index_column":4},{"Column":"Tokens_without_stopwords_lemmatized","Train Accuracy":0.7860262008733624,"Test Accuracy":0.5108695652173914,"Validation Accuracy":0.36363636363636365,"_deepnote_index_column":5}]},"text/plain":"                                Column  Train Accuracy  Test Accuracy  \\\n0                          Tokens_full        0.700873       0.489130   \n1                               Tokens        0.729258       0.478261   \n2             Tokens_without_stopwords        0.884279       0.500000   \n3     Tokens_without_stopwords_stemmed        0.760917       0.543478   \n4                    Tokens_lemmatized        0.633406       0.445652   \n5  Tokens_without_stopwords_lemmatized        0.786026       0.510870   \n\n   Validation Accuracy  \n0             0.272727  \n1             0.272727  \n2             0.363636  \n3             0.454545  \n4             0.363636  \n5             0.363636  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Column</th>\n      <th>Train Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>Validation Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Tokens_full</td>\n      <td>0.700873</td>\n      <td>0.489130</td>\n      <td>0.272727</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Tokens</td>\n      <td>0.729258</td>\n      <td>0.478261</td>\n      <td>0.272727</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Tokens_without_stopwords</td>\n      <td>0.884279</td>\n      <td>0.500000</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Tokens_without_stopwords_stemmed</td>\n      <td>0.760917</td>\n      <td>0.543478</td>\n      <td>0.454545</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Tokens_lemmatized</td>\n      <td>0.633406</td>\n      <td>0.445652</td>\n      <td>0.363636</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tokens_without_stopwords_lemmatized</td>\n      <td>0.786026</td>\n      <td>0.510870</td>\n      <td>0.363636</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"outputs_reference":"s3:deepnote-cell-outputs-production/f1bca23f-a852-41bd-a12f-09edc8f258b6","content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2820e488-6f1b-466d-af14-a66826f012e3' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"c2a6ec1c61a24da2bdde2075668c18a2"}}